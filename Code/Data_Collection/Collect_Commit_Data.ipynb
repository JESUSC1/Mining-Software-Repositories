{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9374b68d-1dda-4028-accc-ba6bda1a2e1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming processing from commit 285ee665ee23b86574d3f4810c0c0654bdc9fdfc\n",
      "Processing repository...\n",
      "25 commits processed\n",
      "50 commits processed\n",
      "75 commits processed\n",
      "100 commits processed\n",
      "125 commits processed\n",
      "150 commits processed\n",
      "175 commits processed\n",
      "200 commits processed\n",
      "225 commits processed\n",
      "250 commits processed\n",
      "275 commits processed\n",
      "300 commits processed\n",
      "325 commits processed\n",
      "350 commits processed\n",
      "375 commits processed\n",
      "400 commits processed\n",
      "425 commits processed\n",
      "450 commits processed\n",
      "475 commits processed\n",
      "500 commits processed\n",
      "525 commits processed\n",
      "550 commits processed\n",
      "575 commits processed\n",
      "600 commits processed\n",
      "625 commits processed\n",
      "650 commits processed\n",
      "675 commits processed\n",
      "700 commits processed\n",
      "725 commits processed\n",
      "750 commits processed\n",
      "775 commits processed\n",
      "800 commits processed\n",
      "825 commits processed\n",
      "850 commits processed\n",
      "875 commits processed\n",
      "900 commits processed\n",
      "925 commits processed\n",
      "950 commits processed\n",
      "975 commits processed\n",
      "1000 commits processed\n",
      "2000 commits processed\n",
      "3000 commits processed\n",
      "4000 commits processed\n",
      "5000 commits processed\n",
      "6000 commits processed\n",
      "7000 commits processed\n",
      "8000 commits processed\n",
      "9000 commits processed\n",
      "10000 commits processed\n",
      "11000 commits processed\n",
      "12000 commits processed\n",
      "13000 commits processed\n",
      "14000 commits processed\n",
      "15000 commits processed\n",
      "16000 commits processed\n",
      "Finished processing repository.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This code extracts commit data from a Git repository using PyDriller \n",
    "and saves it in a CSV file. It sets the path to the repository and the CSV file name and header. \n",
    "If the file exists, it reads the last commit ID from it and continues processing from there. \n",
    "For each commit, it gathers information about the commit and writes it to the CSV file, \n",
    "skipping commits that have already been processed. \n",
    "Finally, it prints progress updates every 25 commits until the first 1000 have been processed,and then every 1000 commits. \n",
    "\n",
    "Script Author: Jesus Cantu \n",
    "\"\"\"\n",
    "\n",
    "from pydriller import Repository\n",
    "import csv\n",
    "import requests\n",
    "import subprocess\n",
    "import os.path\n",
    "\n",
    "# Replace this path with your own repository of interest (this could be a local path or URL!) URL e.g., https://github.com/Anuken/Mindustry\n",
    "# If using a local path clone the repository using the mirror option, e.g., git clone --mirror https://github.com/scikit-learn/scikit-learn.git",
    "repo_url = '/Users/jesuscantu/Desktop/Temp/Cloned_Repos/Mindustry.git' # local path \n",
    "\n",
    "# Set up CSV file and header row\n",
    "file_name = '/Users/jesuscantu/Desktop/Temp/Mindustry_commit_data.csv' # ~/filepath/csv.name\n",
    "header_row = ['commit_id', 'message', 'author_name', 'author_email', 'author_date', 'author_tz', \n",
    "              'committer_name', 'committer_email', 'committer_date', 'committer_tz', 'in_main',\n",
    "              'is_merge', 'num_deletes', 'num_inserts', 'net_lines', 'num_files', 'branches',\n",
    "              'files', 'parents', 'dmm_unit_size', 'dmm_unit_complexity', 'dmm_unit_interfacing']\n",
    "\n",
    "if os.path.isfile(file_name):\n",
    "    # If the file already exists, read the last commit ID from it and continue processing from there\n",
    "    with open(file_name, 'r') as csv_file:\n",
    "        last_commit_id = ''\n",
    "        for row in csv.reader(csv_file):\n",
    "            last_commit_id = str(row[0]) # Convert the commit hash to a string\n",
    "        print(f'Resuming processing from commit {last_commit_id}')\n",
    "        append_mode = 'a'\n",
    "else:\n",
    "    # If the file doesn't exist, start a new one\n",
    "    print('Creating new CSV file')\n",
    "    append_mode = 'w'\n",
    "\n",
    "with open(file_name, mode=append_mode, newline='') as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    if append_mode == 'w':\n",
    "        writer.writerow(header_row)\n",
    "\n",
    "    # Loop over each PyDriller commit to transform it to a commit usable for analysis later\n",
    "    print(\"Processing repository...\")\n",
    "    commit_count = 0\n",
    "    for commit in Repository(repo_url).traverse_commits():\n",
    "        hash = commit.hash\n",
    "\n",
    "        # Skip commits that have already been processed\n",
    "        if append_mode == 'a' and hash == last_commit_id:\n",
    "            continue\n",
    "\n",
    "        # Gather a list of files modified in the commit\n",
    "        files = []\n",
    "        try:\n",
    "            for f in commit.modified_files:\n",
    "                if f.new_path is not None:\n",
    "                    files.append(f.new_path)\n",
    "        except Exception:\n",
    "            print('Could not read files for commit ' + hash)\n",
    "            continue\n",
    "\n",
    "        # Capture information about the commit in object format so I can reference it later\n",
    "        record = {\n",
    "            'commit_id': hash,\n",
    "            'message': commit.msg,\n",
    "            'author_name': commit.author.name,\n",
    "            'author_email': commit.author.email,\n",
    "            'author_date': commit.author_date,\n",
    "            'author_tz': commit.author_timezone,\n",
    "            'committer_name': commit.committer.name,\n",
    "            'committer_email': commit.committer.email,\n",
    "            'committer_date': commit.committer_date,\n",
    "            'committer_tz': commit.committer_timezone,\n",
    "            'in_main': commit.in_main_branch,\n",
    "            'is_merge': commit.merge,\n",
    "            'num_deletes': commit.deletions,\n",
    "            'num_inserts': commit.insertions,\n",
    "            'net_lines': commit.insertions - commit.deletions,\n",
    "            'num_files': commit.files,\n",
    "            'branches': ', '.join(commit.branches), # Comma separated list of branches the commit is found in\n",
    "            'files': ', '.join(files), # Comma separated list of files the commit modifies\n",
    "            'parents': ', '.join(commit.parents), # Comma separated list of parents\n",
    "            # PyDriller Open Source Delta Maintainability Model (OS-DMM) stat.", 
    "            # See https://pydriller.readthedocs.io/en/latest/deltamaintainability.html for metric definitions\n",
    "            'dmm_unit_size': commit.dmm_unit_size,\n",
    "            'dmm_unit_complexity': commit.dmm_unit_complexity,\n",
    "            'dmm_unit_interfacing': commit.dmm_unit_interfacing,\n",
    "        }\n",
    "        writer.writerow(record.values())\n",
    "        \n",
    "        # Print progress every 1000 commits\n",
    "        commit_count += 1\n",
    "        # Print progress every 25 commits and only every 1000 commits after 1000\n",
    "        if (commit_count < 1000 and commit_count % 25 == 0) or commit_count % 1000 == 0:\n",
    "            print(f'{commit_count} commits processed')\n",
    "            \n",
    "    print(\"Finished processing repository.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad142e88-4ca0-4742-9f9f-f1f25ef0446a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4505df9a-804b-40ca-8c2f-1474d0ca2130",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
